import g4f
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

TEST_PROMPT = "hello, how are u?"

MODELS = [
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4o-mini",
    "claude-3-haiku",
    "gemini-pro",
]

EXCLUDE_PROVIDERS = {
    "Openai", "OpenAI", "NeedsAuth", "Anthropic", "AnyProvider", 
    "Azure", "BingCreateImages", "BackendApi", "ApiAirforce", "ARTA",
    "You", "BlackForestLabs", "Chatai", "Flux1Dev", "BaseProvider"
}

def get_available_providers():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    providers = []
    for name in dir(g4f.Provider):
        if name.startswith("_") or not name:
            continue
        if any(exc in name for exc in EXCLUDE_PROVIDERS):
            continue
        try:
            prov = getattr(g4f.Provider, name)
            providers.append((name, prov))
        except Exception:
            continue
    return providers

def test_provider_model(provider_name, provider, model):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –ø—Ä–æ–≤–∞–π–¥–µ—Ä+–º–æ–¥–µ–ª—å"""
    try:
        start_time = time.time()
        response = g4f.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": TEST_PROMPT}],
            provider=provider,
            temperature=0.2,
            timeout=30
        )
        elapsed = time.time() - start_time
        
        if response and len(response.strip()) > 0:
            return {
                "provider": provider_name,
                "model": model,
                "status": "SUCCESS",
                "response_length": len(response.strip()),
                "time": round(elapsed, 2),
                "response_preview": response.strip()[:100] + "..." if len(response.strip()) > 100 else response.strip()
            }
        else:
            return {
                "provider": provider_name,
                "model": model,
                "status": "EMPTY_RESPONSE",
                "time": round(elapsed, 2)
            }
    except Exception as e:
        return {
            "provider": provider_name,
            "model": model,
            "status": "ERROR",
            "error": str(e)[:200]
        }

def test_default_g4f():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π g4f –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    try:
        start_time = time.time()
        response = g4f.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": TEST_PROMPT}],
            temperature=0.2,
            timeout=30
        )
        elapsed = time.time() - start_time
        
        if response and len(response.strip()) > 0:
            return {
                "provider": "DEFAULT_G4F",
                "model": "gpt-3.5-turbo",
                "status": "SUCCESS",
                "response_length": len(response.strip()),
                "time": round(elapsed, 2),
                "response_preview": response.strip()[:100] + "..." if len(response.strip()) > 100 else response.strip()
            }
        else:
            return {
                "provider": "DEFAULT_G4F",
                "model": "gpt-3.5-turbo",
                "status": "EMPTY_RESPONSE",
                "time": round(elapsed, 2)
            }
    except Exception as e:
        return {
            "provider": "DEFAULT_G4F",
            "model": "gpt-3.5-turbo",
            "status": "ERROR",
            "error": str(e)[:200]
        }

def main():
    print("üîç G4F Provider & Model Tester")
    print("=" * 50)
    
    providers = get_available_providers()
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(providers)}")
    print(f"üéØ –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {', '.join(MODELS)}")
    print()
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π g4f...")
    default_result = test_default_g4f()
    print(f"   {default_result['status']}: {default_result.get('response_preview', default_result.get('error', ''))}")
    print()
    
    tasks = []
    for provider_name, provider in providers:
        for model in MODELS:
            tasks.append((provider_name, provider, model))
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞—é {len(tasks)} —Ç–µ—Å—Ç–æ–≤...")
    print()
    
    results = []
    successful = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_task = {
            executor.submit(test_provider_model, pname, prov, model): (pname, model)
            for pname, prov, model in tasks
        }
        
        completed = 0
        for future in as_completed(future_to_task, timeout=300):
            pname, model = future_to_task[future]
            try:
                result = future.result(timeout=35)
                results.append(result)
                
                if result["status"] == "SUCCESS":
                    successful.append(result)
                    print(f"‚úÖ {pname} + {model}: {result['time']}s - {result['response_preview']}")
                else:
                    print(f"‚ùå {pname} + {model}: {result['status']} - {result.get('error', '')}")
                
                completed += 1
                if completed % 10 == 0:
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/{len(tasks)}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  {pname} + {model}: TIMEOUT/ERROR - {str(e)[:100]}")
    
    print()
    print("üìä RESULTS:")
    print("=" * 50)
    
    if successful:
        print(f"‚úÖ –†–∞–±–æ—á–∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(successful)}")
        print()
        
        successful.sort(key=lambda x: x['time'])
        
        print("üèÜ –¢–û–ü-10 –ë–´–°–¢–†–´–• –ò –†–ê–ë–û–ß–ò–•:")
        for i, result in enumerate(successful[:10], 1):
            print(f"{i:2d}. {result['provider']:20} + {result['model']:15} ({result['time']:4.1f}s)")
        
        print()
        print("üîß –ö–û–î –î–õ–Ø –ü–õ–ê–ì–ò–ù–ê:")
        print("priority_providers = [")
        for result in successful[:7]:
            print(f'    "{result["provider"]}",  # {result["time"]}s')
        print("]")
        
        print()
        print("models_fallback = [")
        
        model_counts = {}
        for result in successful:
            model = result["model"]
            model_counts[model] = model_counts.get(model, 0) + 1
        
        sorted_models = sorted(model_counts.items(), key=lambda x: x[1], reverse=True)
        for model, count in sorted_models[:4]:
            print(f'    "{model}",  # —Ä–∞–±–æ—Ç–∞–µ—Ç —É {count} –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤')
        print("]")
        
    else:
        print("‚ùå –†–∞–±–æ—á–∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        if default_result["status"] == "SUCCESS":
            print("üí° –ù–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π g4f —Ä–∞–±–æ—Ç–∞–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ provider=None")
    
    with open("g4f_test_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "default_g4f": default_result,
            "all_results": results,
            "successful": successful,
            "total_tested": len(tasks),
            "success_rate": len(successful) / len(tasks) * 100 if tasks else 0
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ g4f_test_results.json")

if __name__ == "__main__":
    main()
